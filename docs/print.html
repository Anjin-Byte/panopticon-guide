<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Training Agents for Panopticon</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Training Agents for Panopticon</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h3 id="introduction"><a class="header" href="#introduction"><strong>Introduction</strong></a></h3>
<p>Imagine orchestrating complex scenarios where aircraft, ships, and facilities interact in dynamic and unpredictable environments. What if you could build an intelligent agent capable of navigating these challenges, learning strategies, and optimizing performance in real time? Enter Panopticon, a cutting-edge simulation platform that fuses realism with the flexibility of OpenAI Gym.</p>
<p>In this guide, we'll show you how to unlock the full potential of Panopticon by leveraging its seamless integration with reinforcement learning frameworks like Stable-Baselines3. You'll learn how to create environments, design observation and action spaces, and train agents to master complex scenarios.</p>
<p>Let’s dive in and start building a simple aircraft agent!</p>
<div style="break-before: page; page-break-before: always;"></div><h3 id="layout-of-blade-and-its-interface-with-openai-gymnasium"><a class="header" href="#layout-of-blade-and-its-interface-with-openai-gymnasium"><em>Layout of BLADE and Its Interface with OpenAI Gymnasium</em></a></h3>
<p>To effectively use the <strong>BLADE</strong> environment with OpenAI Gymnasium, it’s essential to understand its architecture and how it bridges the simulation features of Panopticon with the modular reinforcement learning ecosystem of Gym. Here’s a structured breakdown:</p>
<pre class="mermaid">sequenceDiagram
    participant Train_py as train.py
    participant Game as Game Object
    participant Scenario as Scenario
    participant Gym as Gym Environment
    participant Agent as RL Agent

    Train_py-&gt;&gt;Game: Initialize Game with Scenario
    Train_py-&gt;&gt;Scenario: Load Scenario from JSON
    Train_py-&gt;&gt;Gym: Define Observation and Action Spaces
    Train_py-&gt;&gt;Gym: Set Transformation Functions (Action, Observation, Reward, Termination)
    Train_py-&gt;&gt;Gym: Initialize Gym Environment with Game

    loop Training Loop
        Agent-&gt;&gt;Gym: Take Action
        Gym-&gt;&gt;Game: Transform Action to Command
        Game-&gt;&gt;Scenario: Update Scenario State
        Scenario-&gt;&gt;Game: Adjust Entity Positions
        Scenario-&gt;&gt;Game: Update Fuel and Resources
        Scenario-&gt;&gt;Game: Resolve Engagements
        Scenario-&gt;&gt;Game: Update Mission Objectives
        Game-&gt;&gt;Gym: Return Updated State
        Gym-&gt;&gt;Agent: Provide Observation, Reward, and Termination Status
    end

    Train_py-&gt;&gt;Agent: Evaluate Trained Agent
    Agent-&gt;&gt;Gym: Interact with Environment for Evaluation
    Gym-&gt;&gt;Train_py: Export Scenario State for Debugging
</pre>
<h3 id="blade-layout"><a class="header" href="#blade-layout"><strong>BLADE Layout</strong></a></h3>
<p>The BLADE environment acts as a wrapper that integrates the simulation capabilities of the Panopticon system with the reinforcement learning framework. Here’s how its key components are organized:</p>
<ol>
<li>
<p><strong>Simulation Backbone</strong></p>
<ul>
<li><strong>Game</strong>: Manages the current state of the simulation, including entities (e.g., aircraft, ships, facilities) and missions.
<ul>
<li>Provides the <code>step</code> method, which processes actions, updates the simulation state, and returns observations, rewards, and termination conditions.</li>
</ul>
</li>
<li><strong>Scenario</strong>: Represents the configuration of a specific simulation, including the entities, sides, and missions involved.</li>
</ul>
</li>
<li>
<p><strong>Entities and Missions</strong></p>
<ul>
<li><strong>Entities</strong>: Aircraft, ships, facilities, and airbases with customizable properties like position, speed, and weapons.</li>
<li><strong>Missions</strong>: Goals assigned to entities, such as patrols or strikes, which guide their behavior within the scenario.</li>
</ul>
</li>
<li>
<p><strong>Utility Modules</strong></p>
<ul>
<li>Constants and utility functions to calculate distances, headings, and other spatial dynamics.</li>
<li>Filtering and logging mechanisms to extract or record meaningful data.</li>
</ul>
</li>
</ol>
<h3 id="interface-with-openai-gymnasium"><a class="header" href="#interface-with-openai-gymnasium"><strong>Interface with OpenAI Gymnasium</strong></a></h3>
<p>BLADE leverages the Gymnasium API to create a reinforcement learning environment. Here's how it connects the dots:</p>
<ol>
<li>
<p><strong>Environment Initialization</strong></p>
<ul>
<li>The <code>BLADE</code> class inherits from <code>gym.Env</code> and serves as the Gym-compatible environment.</li>
<li>During initialization:
<ul>
<li>The <code>Game</code> object is provided to manage the simulation.</li>
<li>Observation and action spaces are defined, either with defaults or user customization.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Key Gym API Methods</strong></p>
<ul>
<li><strong><code>reset()</code></strong>:
<ul>
<li>Resets the simulation to its initial state using the <code>Game</code> class.</li>
<li>Returns an initial observation and optional metadata.</li>
</ul>
</li>
<li><strong><code>step(action)</code></strong>:
<ul>
<li>Accepts an action, transforms it for the simulation, and applies it to the <code>Game</code> object.</li>
<li>Updates the simulation state and retrieves:
<ul>
<li>The next observation (<code>observation_filter_fnc</code> is applied if defined).</li>
<li>A reward value (<code>reward_filter_fnc</code>).</li>
<li>Termination status (<code>termination_filter_fnc</code>).</li>
<li>Additional info for debugging or diagnostics.</li>
</ul>
</li>
</ul>
</li>
<li><strong><code>render()</code></strong> (Optional):
<ul>
<li>Provides visual feedback, useful for debugging or demonstrations.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Customizable Interfaces</strong></p>
<ul>
<li><strong>Action Transformation</strong>: <code>action_transform_fnc</code> allows the user to map RL agent outputs (actions) into commands understood by the simulation, such as moving an aircraft.</li>
<li><strong>Observation Filtering</strong>: <code>observation_filter_fnc</code> tailors the simulation state into meaningful observations for the RL agent, like the position of an aircraft or proximity to threats.</li>
<li><strong>Reward and Termination</strong>:
<ul>
<li><code>reward_filter_fnc</code>: Defines how agent performance is evaluated.</li>
<li><code>termination_filter_fnc</code>: Sets conditions for ending an episode.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Gym Spaces</strong></p>
<ul>
<li><strong>Observation Space</strong>: Describes the format and limits of data returned to the agent, e.g., aircraft latitude and longitude.</li>
<li><strong>Action Space</strong>: Specifies the range and type of actions the agent can take, e.g., target coordinates for a movement command.</li>
</ul>
</li>
</ol>
<h3 id="how-it-all-fits-together"><a class="header" href="#how-it-all-fits-together"><strong>How It All Fits Together</strong></a></h3>
<p>When you combine BLADE with Gymnasium, you get an environment that acts as a bridge between the high-fidelity simulation of Panopticon and the algorithms provided by reinforcement learning libraries like Stable-Baselines3. Here's the flow:</p>
<ol>
<li>The agent interacts with the BLADE environment via the Gymnasium API.</li>
<li>Actions from the agent are transformed and applied to the simulation (<code>Game.step()</code>).</li>
<li>The simulation updates its state, calculates rewards, and checks for termination.</li>
<li>Filtered observations and rewards are returned to the agent, closing the loop.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h3 id="setting-up-your-environment"><a class="header" href="#setting-up-your-environment"><strong>Setting Up Your Environment</strong></a></h3>
<div style="break-before: page; page-break-before: always;"></div><h3 id="designing-observations-and-actions"><a class="header" href="#designing-observations-and-actions"><strong>Designing Observations and Actions</strong></a></h3>
<div style="break-before: page; page-break-before: always;"></div><h3 id="detailed-explanation-of-the-reward-function"><a class="header" href="#detailed-explanation-of-the-reward-function"><strong>Detailed Explanation of the Reward Function</strong></a></h3>
<p>The reward function is the heart of reinforcement learning. It translates the results of an agent's actions into numerical feedback that the agent uses to learn. In this scenario, the reward function is designed to:</p>
<ol>
<li>Encourage the aircraft to reduce its distance to the target location.</li>
<li>Align its heading with the optimal trajectory toward the goal.</li>
<li>Penalize erratic heading changes to ensure smooth movement.</li>
</ol>
<hr />
<h3 id="1-distance-progress-reward"><a class="header" href="#1-distance-progress-reward"><strong>1. Distance Progress Reward</strong></a></h3>
<h4 id="purpose"><a class="header" href="#purpose"><strong>Purpose</strong></a></h4>
<p>This reward encourages the aircraft to minimize its distance to the target. The closer it gets to the goal, the higher the reward.</p>
<h4 id="how-it-works"><a class="header" href="#how-it-works"><strong>How It Works</strong></a></h4>
<ol>
<li>
<p><strong>Calculate Distance</strong>:
The function calculates the Euclidean distance from the aircraft’s previous position to the goal (<code>previous_distance</code>) and from its current position to the goal (<code>current_distance</code>).</p>
<pre><code class="language-python">previous_distance = euclidean_distance(
    [previous_latitude, previous_longitude], goal_coordinates
)
current_distance = euclidean_distance(
    [aircraft.latitude, aircraft.longitude], goal_coordinates
)
</code></pre>
</li>
<li>
<p><strong>Compute Reward</strong>:
The difference between the previous and current distances determines progress:</p>
<pre><code class="language-python">distance_progress_reward = scaling_factors.get("distance") * (
    previous_distance - current_distance
)
</code></pre>
<p>If the aircraft moves closer to the goal, the reward is positive. Moving away results in a negative reward.</p>
</li>
<li>
<p><strong>Scaling Factor</strong>:
A scaling factor (<code>scaling_factors.get("distance")</code>) adjusts the reward’s magnitude to influence the agent's learning.</p>
</li>
</ol>
<h4 id="why-its-important"><a class="header" href="#why-its-important"><strong>Why It’s Important</strong></a></h4>
<ul>
<li><strong>Encourages Goal-Oriented Behavior</strong>: Guides the agent to prioritize moving toward the target.</li>
<li><strong>Reward Alignment</strong>: Reflects progress in terms of simulation objectives.</li>
</ul>
<hr />
<h3 id="2-heading-alignment-reward"><a class="header" href="#2-heading-alignment-reward"><strong>2. Heading Alignment Reward</strong></a></h3>
<h4 id="purpose-1"><a class="header" href="#purpose-1"><strong>Purpose</strong></a></h4>
<p>This reward ensures the aircraft aligns its heading with the optimal trajectory toward the goal. Misaligned headings waste time and fuel, so this reward encourages efficiency.</p>
<h4 id="how-it-works-1"><a class="header" href="#how-it-works-1"><strong>How It Works</strong></a></h4>
<ol>
<li>
<p><strong>Calculate Target Heading</strong>:
The target heading is the angle between the aircraft’s current position and the goal:</p>
<pre><code class="language-python">target_heading = get_bearing_between_two_points(
    aircraft.latitude, aircraft.longitude, goal_coordinates[0], goal_coordinates[1]
)
</code></pre>
</li>
<li>
<p><strong>Calculate Heading Difference</strong>:
The difference between the aircraft’s current heading (<code>previous_heading</code>) and the target heading is normalized to a range of 0°–180°:</p>
<pre><code class="language-python">heading_difference = abs((previous_heading - target_heading + 180) % 360 - 180)
</code></pre>
</li>
<li>
<p><strong>Compute Reward</strong>:
The reward decreases linearly with the heading difference:</p>
<pre><code class="language-python">heading_alignment_reward = (1 - heading_difference / 180.0) * scaling_factors.get("heading_align")
</code></pre>
<p>A perfectly aligned heading yields the maximum reward, while a completely opposite heading (180° off) results in no reward.</p>
</li>
<li>
<p><strong>Scaling Factor</strong>:
A scaling factor (<code>scaling_factors.get("heading_align")</code>) adjusts the importance of alignment relative to other rewards.</p>
</li>
</ol>
<h4 id="why-its-important-1"><a class="header" href="#why-its-important-1"><strong>Why It’s Important</strong></a></h4>
<ul>
<li><strong>Promotes Efficiency</strong>: Aligning with the optimal trajectory minimizes travel time and fuel consumption.</li>
<li><strong>Prevents Erratic Behavior</strong>: Discourages the agent from taking unnecessary detours.</li>
</ul>
<hr />
<h3 id="3-heading-smoothness-penalty"><a class="header" href="#3-heading-smoothness-penalty"><strong>3. Heading Smoothness Penalty</strong></a></h3>
<h4 id="purpose-2"><a class="header" href="#purpose-2"><strong>Purpose</strong></a></h4>
<p>This component penalizes erratic heading changes, encouraging the agent to maintain a smooth trajectory.</p>
<h4 id="how-it-works-2"><a class="header" href="#how-it-works-2"><strong>How It Works</strong></a></h4>
<ol>
<li>
<p><strong>Extract Heading History</strong>:
The function retrieves a history of the aircraft’s headings from its black box logs:</p>
<pre><code class="language-python">headings = [
    log_entry["heading"]
    for log_entry in aircraft.black_box._logs
    if "heading" in log_entry
]
</code></pre>
</li>
<li>
<p><strong>Calculate Smoothness</strong>:
The mean squared change in heading is calculated to quantify smoothness:</p>
<pre><code class="language-python">differences = np.abs(np.diff(headings))
differences = np.where(differences &gt; 180, 360 - differences, differences)
mean_squared_change = np.mean(differences**2)
</code></pre>
</li>
<li>
<p><strong>Compute Penalty</strong>:
The penalty is proportional to the mean squared change:</p>
<pre><code class="language-python">heading_smoothness_penalty = -mean_squared_change * scaling_factors.get("heading_smoothness")
</code></pre>
</li>
</ol>
<h4 id="why-its-important-2"><a class="header" href="#why-its-important-2"><strong>Why It’s Important</strong></a></h4>
<ul>
<li><strong>Encourages Stability</strong>: Prevents abrupt heading changes that could indicate inefficient or erratic behavior.</li>
<li><strong>Promotes Realism</strong>: Smooth trajectories are more realistic for an aircraft.</li>
</ul>
<hr />
<h3 id="4-exponential-reward-optional"><a class="header" href="#4-exponential-reward-optional"><strong>4. Exponential Reward (Optional)</strong></a></h3>
<h4 id="purpose-3"><a class="header" href="#purpose-3"><strong>Purpose</strong></a></h4>
<p>This optional reward component emphasizes proximity to the goal, providing higher rewards as the aircraft nears the target.</p>
<h4 id="how-it-works-3"><a class="header" href="#how-it-works-3"><strong>How It Works</strong></a></h4>
<ol>
<li>
<p><strong>Calculate Exponential Reward</strong>:
The reward exponentially increases as the distance to the goal decreases:</p>
<pre><code class="language-python">exponential_reward = np.exp(-current_distance) * scaling_factors.get("exp_progress")
</code></pre>
</li>
<li>
<p><strong>Incentivizing the Goal</strong>:
This component makes it highly rewarding for the aircraft to approach the target quickly.</p>
</li>
</ol>
<h4 id="why-its-important-3"><a class="header" href="#why-its-important-3"><strong>Why It’s Important</strong></a></h4>
<ul>
<li><strong>Fine-Tuned Precision</strong>: Adds another layer of guidance near the goal.</li>
<li><strong>Optional Flexibility</strong>: Can be toggled on or off depending on the desired behavior.</li>
</ul>
<hr />
<h3 id="5-combining-reward-components"><a class="header" href="#5-combining-reward-components"><strong>5. Combining Reward Components</strong></a></h3>
<p>The total reward is a weighted sum of all components:</p>
<pre><code class="language-python">total_reward = (
    distance_progress_reward
    + heading_alignment_reward
    + heading_smoothness_penalty
    # + exponential_reward (optional)
)
</code></pre>
<p>Each component is scaled to balance its influence:</p>
<ul>
<li><strong>Distance</strong>: Encourages consistent progress toward the goal.</li>
<li><strong>Alignment</strong>: Promotes heading efficiency.</li>
<li><strong>Smoothness</strong>: Discourages abrupt or erratic movements.</li>
</ul>
<hr />
<h3 id="7-why-this-reward-function-is-effective"><a class="header" href="#7-why-this-reward-function-is-effective"><strong>7. Why This Reward Function is Effective</strong></a></h3>
<ol>
<li>
<p><strong>Multi-Faceted Guidance</strong>:</p>
<ul>
<li>Combines different aspects of aircraft behavior into a single numerical feedback loop.</li>
</ul>
</li>
<li>
<p><strong>Realism and Efficiency</strong>:</p>
<ul>
<li>Simulates real-world considerations like fuel efficiency and stable trajectories.</li>
</ul>
</li>
<li>
<p><strong>Customizability</strong>:</p>
<ul>
<li>Developers can adjust scaling factors or toggle components (e.g., exponential reward) to fine-tune agent behavior.</li>
</ul>
</li>
<li>
<p><strong>Debug-Friendly</strong>:</p>
<ul>
<li>Breaks the reward into understandable parts, making it easier to identify and fix issues.</li>
</ul>
</li>
</ol>
<hr />
<div style="break-before: page; page-break-before: always;"></div><h3 id="training-your-first-agent"><a class="header" href="#training-your-first-agent"><strong>Training Your First Agent</strong></a></h3>
<div style="break-before: page; page-break-before: always;"></div><h3 id="evaluating-agent-performance"><a class="header" href="#evaluating-agent-performance"><strong>Evaluating Agent Performance</strong></a></h3>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->
        <script src="mermaid.min.js"></script>
        <script src="mermaid-init.js"></script>

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
